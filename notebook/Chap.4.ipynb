{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ニューラルネットワークの学習\n",
    "\n",
    "### 学習とは\n",
    "\n",
    "p.83\n",
    "\n",
    "> ここで言う「学習」とは、訓練データから最適な重みパラメータの値を自動で獲得することを指します。\n",
    "\n",
    "### 損失関数\n",
    "\n",
    "ニューラルネットワークが学習を行うための**指標**となる関数。\n",
    "\n",
    "\n",
    "**損失関数が最も小さくなるパラメータを探す**のが学習の目的\n",
    "\n",
    "### 勾配法\n",
    "\n",
    "できるだけ小さな損失関数を探し出すための手法\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データから学習する\n",
    "\n",
    "- ニューラルネットワークの特徴は**データから学習する**こと\n",
    "\n",
    "- 重みパラメータの値をデータから自動で決定する\n",
    "\n",
    "### ニューラルネットワークとパーセプトロンの違い\n",
    "\n",
    "パーセプトロンは線形分離可能な問題であれば、データから自動で学習することができる。非線形分離問題は自動で学習することはできない。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ駆動\n",
    "\n",
    "機械学習がデータを学習するためには、データの特徴量が必要になる。この特徴量は人間が設計したもの。\n",
    "\n",
    "ニューラルネットワークでは人間が特徴量を設計すること無く、データ(例えば画像)をそのまま学習する。\n",
    "\n",
    "ニューラルネットワークは与えられたデータからパターンを発見しようとする。つまり対象とする問題によらずデータをそのまま生データとして学習できる。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練データとテストデータ\n",
    "\n",
    "機械学習の問題では、データを訓練データとテストデータに分ける。\n",
    "\n",
    "訓練していないデータ以外での性能「**汎化能力**」を評価するために、テストデータが必要になる。\n",
    "\n",
    "あるデータセットにだけ適応した状態を「**過学習**(overfitting)」と呼ぶ。過学習を避けることが機械学習の重要な課題。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 損失関数\n",
    "\n",
    "**損失関数**(loss function)はニューラルネットワークの**性能の悪さ**を示す指標。現在のニューラルネットワークが教師データに対して適合していないということを表す。損失関数を最小にする。\n",
    "\n",
    "損失関数にマイナスを掛けた値が「性能が悪くないか」を表す指標として解釈できる。\n",
    "\n",
    "一般的には2乗和誤差や交差エントロピー誤差が用いられる。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2乗和誤差\n",
    "\n",
    "**2乗和誤差**(mean squared error)\n",
    "\n",
    "\\begin{equation}\n",
    "E = \\frac{1}{2}\\sum_{k}(y_{k}-t_{k})^2\n",
    "\\end{equation}\n",
    "\n",
    "- *yk* はニューラルネットワークの出力\n",
    "- *tk*は教師データ\n",
    "- kはデータの次元数を表す。\n",
    "\n",
    "### 「手書き数字認識」の例\n",
    "\n",
    "```\n",
    ">>> y = [0.1, 0.05, 0.6, 0.0, 0.05 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    ">>> t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "```\n",
    "\n",
    "これらの配列の要素は最初のインデックスから順に「0」、「1」, ...,「9」に対応する。\n",
    "\n",
    "ニューラルネットワークの出力である*y*はソフトマックス関数の出力。「0」の確率は0.1、「1」の確率は0.05と解釈する。\n",
    "\n",
    "tは教師データ。正解となるラベルを1、それ以外を0とする。\n",
    "\n",
    "### Pythonで実装する\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "\n",
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09750000000000003\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "「2」を正解とする教師データ\n",
    "\"\"\"\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "\"\"\"\n",
    "「2」の確率が最も高い場合のニューラルネットワークの出力\n",
    "\"\"\"\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "\n",
    "print(mean_squared_error(np.array(y), np.array(t)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5975\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "「7」の確率が最も高い場合のニューラルネットワークの出力\n",
    "\"\"\"\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "print(mean_squared_error(np.array(y), np.array(t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "「2」が正解のときの、「2」である確率が高いという出力と「7」である確率が高いという出力では、前者の損失関数の方が小さくなっている。\n",
    "\n",
    "**損失関数が小さい = 教師データ**との誤差が小さい。つまり一つ目の例の方が、出力結果が教師データに適合している。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 交差エントロピー誤差\n",
    "\n",
    "**交差エントロピー誤差**(cross entropy error)\n",
    "\n",
    "\\begin{equation}\n",
    "E = - \\sum_{k}t_{k}\\log{y}_{k}\n",
    "\\end{equation}\n",
    "\n",
    "- logは低が*e*の自然対数を表す\n",
    "\n",
    "- *yk*はニューラルネットワークの出力\n",
    "\n",
    "- *tk*は正解ラベル。正解ラベルのインデックスだけが1、それ以外は0とする。\n",
    "\n",
    "→ 正解ラベルが1に対応する出力の自然対数を計算するだけになる。\n",
    "\n",
    "\n",
    "### 例.\n",
    "「2」が正解ラベルのインデックスとする\n",
    "\n",
    "- 対応するニューラルネットワークの出力が0.6の場合\n",
    "\n",
    "\\begin{equation}\n",
    "-\\log{0.6} = 0.51\n",
    "\\end{equation}\n",
    "\n",
    "- 対応するニューラルネットワークの出力が0.1の場合\n",
    "\n",
    "\\begin{equation}\n",
    "-\\log{0.1} = 2.30\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "交差エントロピー誤差は、正解ラベルとなる出力の結果によって値が決まる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
